{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQi3aTS0Y4Nw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDE2KMrQIR6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Referred to viswanathgs git 'simple_dqn.py' at https://gist.github.com/viswanathgs/abe4a8732a81c666af8bb99254b8e1da\n",
        "# for defining loss function with target newtwork for DQN \n",
        "\n",
        "class DQN:\n",
        "    np.random.seed(1402)\n",
        "    REPLAY_MEMORY_SIZE = 2    # number of tuples in experience replay\n",
        "    EPSILON = 1       # epsilon of epsilon-greedy exploation\n",
        "    EPSILON_DECAY = 0.995   # exponential decay multiplier for epsilon\n",
        "    MIN_EPSILON = 0.01  # Final minimum value of epsilon in epsilon-greedy\n",
        "    HIDDEN1_SIZE = 64      # size of hidden layer 1\n",
        "    HIDDEN2_SIZE = 64     # size of hidden layer 2\n",
        "    HIDDEN3_SIZE = 64      # size of hidden layer 3\n",
        "    EPISODES_NUM = 10000  # number of episodes to train on. Ideally shouldn't take longer than 2000\n",
        "    MAX_STEPS = 205      # maximum number of steps in an episode\n",
        "    LEARNING_RATE = 0.001     # learning rate and other parameters for SGD/RMSProp/Adam\n",
        "    MOMENTUM = 0.95\n",
        "    MINIBATCH_SIZE = 1    # size of minibatch sampled from the experience replay\n",
        "    DISCOUNT_FACTOR = 0.999     # MDP's gamma\n",
        "    TARGET_UPDATE_FREQ = 200    # number of steps (not episodes) after which to update the target networks\n",
        "    LOG_DIR = 'tmp/logs'      # directory wherein logging takes place\n",
        "    LOG_DIR2 = 'tmp/logs2'  # tensorboard --logdir tmp/logs2\n",
        "    REGULARIZATION_FACTOR = 0.0001\n",
        "    replay_memory = []\n",
        "\n",
        "    # Create and initialize the environment\n",
        "    def __init__(self, env):\n",
        "        self.env = gym.make(env)\n",
        "        assert len(self.env.observation_space.shape) == 1\n",
        "        # In case of cartpole, 4 state features\n",
        "        self.input_size = self.env.observation_space.shape[0]\n",
        "        # In case of cartpole, 2 actions (right/left)\n",
        "        self.output_size = self.env.action_space.n\n",
        "        self.load_model = False\n",
        "        self.session = tf.Session()\n",
        "\n",
        "    # Create the Q-network\n",
        "    def initialize_network(self):\n",
        "\n",
        "        # placeholder for the state-space input to the q-network\n",
        "        tf.set_random_seed(1673)\n",
        "        if self.load_model:\n",
        "            new_saver = tf.train.import_meta_graph('models/my_model.meta')\n",
        "            new_saver.restore(self.session,\n",
        "                              tf.train.latest_checkpoint('models/./'))\n",
        "            print(\"Model and data loaded \\n\")\n",
        "        else:\n",
        "            self.x = tf.placeholder(\n",
        "                tf.float32, [None, self.input_size], name=\"x\")\n",
        "            self.episode_length = tf.placeholder(\n",
        "                \"float\", name=\"episode_length\")\n",
        "\n",
        "            ############################################################\n",
        "            # Design your q-network here.\n",
        "            #\n",
        "            # Add hidden layers and the output layer. For instance:\n",
        "            #\n",
        "            # with tf.name_scope('output'):\n",
        "            #\tW_n = tf.Variable(\n",
        "            # \t\t\t tf.truncated_normal([self.HIDDEN_n-1_SIZE, self.output_size],\n",
        "            # \t\t\t stddev=0.01), name='W_n')\n",
        "            # \tb_n = tf.Variable(tf.zeros(self.output_size), name='b_n')\n",
        "            # \tself.Q = tf.matmul(h_n-1, W_n) + b_n\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Your code here\n",
        "            with tf.name_scope('hidden1'):\n",
        "                W1 = tf.Variable(tf.random_uniform(\n",
        "                    [self.input_size, self.HIDDEN1_SIZE], -1.0, 1.0), name='W1')\n",
        "                b1 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE], -1.0, 1.0), name='b1')\n",
        "                h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n",
        "\n",
        "            with tf.name_scope('hidden2'):\n",
        "                W2 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE, self.HIDDEN2_SIZE], -1.0, 1.0), name='W2')\n",
        "                b2 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE], -1.0, 1.0), name='b2')\n",
        "                h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "#             with tf.name_scope('hidden3'):\n",
        "#                 W3 = tf.Variable(tf.random_uniform(\n",
        "#                     [self.HIDDEN2_SIZE, self.HIDDEN3_SIZE], -1.0, 1.0), name='W3')\n",
        "#                 b3 = tf.Variable(tf.random_uniform(\n",
        "#                     [self.HIDDEN2_SIZE], -1.0, 1.0), name='b3')\n",
        "#                 h3 = tf.nn.relu(tf.matmul(h2, W3) + b3)\n",
        "\n",
        "            with tf.name_scope('output'):\n",
        "                W4 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN3_SIZE, self.output_size], -1.0, 1.0), name='W4')\n",
        "                b4 = tf.Variable(tf.random_uniform(\n",
        "                    [self.output_size], -1.0, 1.0), name='b4')\n",
        "                # , name=\"Q_values\")\n",
        "                self.Q = tf.squeeze(tf.matmul(h2, W4) + b4)\n",
        "\n",
        "            self.weights = [W1, b1, W2, b2,  W4, b4]\n",
        "\n",
        "            ############################################################\n",
        "            # Next, compute the loss.\n",
        "            #\n",
        "            # First, compute the q-values. Note that you need to calculate these\n",
        "            # for the actions in the (s,a,s',r) tuples from the experience replay's minibatch\n",
        "            #\n",
        "            # Next, compute the l2 loss between these estimated q-values and\n",
        "            # the target (which is computed using the frozen target network)\n",
        "            #\n",
        "            ############################################################\n",
        "\n",
        "            # Your code here\n",
        "            self.targetQ = tf.placeholder(tf.float32, [None])\n",
        "            self.targetActionMask = tf.placeholder(\n",
        "                tf.float32, [None, self.output_size])\n",
        "            q_values = tf.reduce_sum(tf.multiply(\n",
        "                self.Q, self.targetActionMask), reduction_indices=[1])\n",
        "            self.loss = tf.reduce_mean(\n",
        "                tf.square(tf.subtract(q_values, self.targetQ)))\n",
        "\n",
        "            # # Regularization\n",
        "            for w in [W1, W2]:\n",
        "                self.loss += self.REGULARIZATION_FACTOR * \\\n",
        "                    tf.reduce_sum(tf.square(w))\n",
        "\n",
        "                ############################################################\n",
        "                # Finally, choose a gradient descent algorithm : SGD/RMSProp/Adam.\n",
        "                #\n",
        "                # For instance:\n",
        "                # optimizer = tf.train.GradientDescentOptimizer(self.LEARNING_RATE)\n",
        "                # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                # self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n",
        "                global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
        "\n",
        "        ############################################################\n",
        "\n",
        "    def train(self, episodes_num=EPISODES_NUM):\n",
        "\n",
        "        # Initialize the TF session\n",
        "        saver = tf.train.Saver()\n",
        "        self.episode_length = 0\n",
        "\n",
        "        # Initialize summary for TensorBoard\n",
        "        tf.summary.scalar('loss',self.loss)\n",
        "        tf.summary.scalar('Episode Length', self.episode_length)\n",
        "        self.summary = tf.summary.merge_all()\n",
        "        self.summary_writer = tf.summary.FileWriter(self.LOG_DIR, self.session.graph)\n",
        "\n",
        "\n",
        "        # summary_writer1 = tf.summary.FileWriter(self.LOG_DIR2)\n",
        "        # summary1 = tf.Summary()\n",
        "\n",
        "        # Alternatively, you could use animated real-time plots from matplotlib\n",
        "        # (https://stackoverflow.com/a/24228275/3284912)\n",
        "\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "        # saver = tf.train.Saver()\n",
        "        saver.save(self.session, 'models/my_model')\n",
        "\n",
        "        ############################################################\n",
        "        # Initialize other variables (like the replay memory)\n",
        "        ############################################################\n",
        "\n",
        "        # Your code here\n",
        "        total_steps = 0\n",
        "        step_counts = []\n",
        "        avg_100_steps = []\n",
        "\n",
        "        # Copying weight to target network\n",
        "        target_weights = self.session.run(self.weights)\n",
        "\n",
        "        ############################################################\n",
        "        # Main training loop\n",
        "        #\n",
        "        # In each episode,\n",
        "        #\tpick the action for the given state,\n",
        "        #\tperform a 'step' in the environment to get the reward and next state,\n",
        "        #\tupdate the replay buffer,\n",
        "        #\tsample a random minibatch from the replay buffer,\n",
        "        # \tperform Q-learning,\n",
        "        #\tupdate the target network, if required.\n",
        "        #\n",
        "        #\n",
        "        #\n",
        "        # You'll need to write code in various places in the following skeleton\n",
        "        #\n",
        "        ############################################################\n",
        "\n",
        "        for episode in tqdm.tqdm(range(episodes_num)):\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            ############################################################\n",
        "            # Episode-specific initializations go here.\n",
        "            ############################################################\n",
        "            #\n",
        "            # Your code here\n",
        "            #\n",
        "            self.episode_length = 0\n",
        "            # done = False\n",
        "\n",
        "            ############################################################\n",
        "            # (done == False) and (episode_length < self.MAX_STEPS):\n",
        "            while True:\n",
        "                # print(done)\n",
        "                # for step in range(self.MAX_STEPS):\n",
        "                ############################################################\n",
        "                # Pick the next action using epsilon greedy and and execute it\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                action = None\n",
        "                if self.EPSILON > np.random.rand():\n",
        "                    a = self.env.action_space.sample()\n",
        "                else:\n",
        "                    q_values = self.session.run(\n",
        "                        self.Q, feed_dict={self.x: [state]})\n",
        "                    a = q_values.argmax()\n",
        "\n",
        "                # Decaying epsilon\n",
        "                self.EPSILON *= self.EPSILON_DECAY\n",
        "\n",
        "\n",
        "                ############################################################\n",
        "                # Step in the environment. Something like:\n",
        "                # next_state, reward, done, _ = self.env.step(action)\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                next_state, reward, done, _ = self.env.step(a)\n",
        "\n",
        "                ############################################################\n",
        "                # Update the (limited) replay buffer.\n",
        "                #\n",
        "                # Note : when the replay buffer is full, you'll need to\n",
        "                # remove an entry to accommodate a new one.\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                if done:\n",
        "                    reward = -100\n",
        "\n",
        "                self.replay_memory.append(\n",
        "                    (state, a, reward, next_state, done))\n",
        "\n",
        "                # removing an entry if replay buffer is full\n",
        "                if len(self.replay_memory) >= self.REPLAY_MEMORY_SIZE:\n",
        "                    self.replay_memory.pop(0)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                ############################################################\n",
        "                # Sample a random minibatch and perform Q-learning (fetch max Q at s')\n",
        "                #\n",
        "                # Remember, the target (r + gamma * max Q) is computed\n",
        "                # with the help of the target network.\n",
        "                # Compute this target and pass it to the network for computing\n",
        "                # and minimizing the loss with the current estimates\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                if len(self.replay_memory) >= self.MINIBATCH_SIZE:\n",
        "                    minibatch = random.sample(\n",
        "                        self.replay_memory, self.MINIBATCH_SIZE )\n",
        "                    next_states = [s[3] for s in minibatch]\n",
        "\n",
        "                    feed_dict = {self.x: next_states}\n",
        "                    feed_dict.update(zip(self.weights, target_weights))\n",
        "                    q_values = self.session.run(self.Q, feed_dict=feed_dict)\n",
        "                    max_q_values = q_values.max(axis=1)\n",
        "\n",
        "                    # Computing Target Q Values\n",
        "                    target_q = np.zeros(self.MINIBATCH_SIZE)\n",
        "                    target_action_mask = np.zeros(\n",
        "                        (self.MINIBATCH_SIZE, self.output_size), dtype=int)\n",
        "                    for i in range(self.MINIBATCH_SIZE):\n",
        "                        st, action, reward, n_st, completed = minibatch[i]\n",
        "                        target_q[i] = reward\n",
        "                        if not completed:\n",
        "                            target_q[i] += self.DISCOUNT_FACTOR*max_q_values[i]\n",
        "                        target_action_mask[i][action] = 1\n",
        "\n",
        "                    # Performing gradient descent\n",
        "                    states = [s[0] for s in minibatch]\n",
        "                    feed_dict = {\n",
        "                        self.x: states,\n",
        "                        self.targetQ: target_q,\n",
        "                        self.targetActionMask: target_action_mask,\n",
        "                    }\n",
        "                    _, summary2 = self.session.run(\n",
        "                        [self.train_op, self.summary], feed_dict=feed_dict)\n",
        "                    \n",
        "                    ############################################################\n",
        "                    # Update target weights.\n",
        "                    #\n",
        "                    # Something along the lines of:\n",
        "                    # if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
        "                    # \ttarget_weights = self.session.run(self.weights)\n",
        "                    ############################################################\n",
        "\n",
        "                    # Your code here\n",
        "                    if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
        "                        target_weights = self.session.run(self.weights)\n",
        "                        #print(\"Targets updated \\n\")\n",
        "\n",
        "                    if total_steps % 100 == 0:\n",
        "                        self.summary_writer.add_summary(summary2, episode)\n",
        "\n",
        "\n",
        "                        # Save the variables to disk.\n",
        "                        # save_path = saver.save(self.session, \"/models/model.ckpt\")\n",
        "                        # print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "\n",
        "                ############################################################\n",
        "                # Break out of the loop if the episode ends\n",
        "                #\n",
        "                # Something like:\n",
        "                # if done or (episode_length == self.MAX_STEPS):\n",
        "                # \tbreak\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                total_steps += 1\n",
        "                self.episode_length += 1\n",
        "\n",
        "                if done or (self.episode_length == self.MAX_STEPS):\n",
        "                    break\n",
        "\n",
        "            ############################################################\n",
        "            # Logging.\n",
        "            #\n",
        "            # Very important. This is what gives an idea of how good the current\n",
        "            # experiment is, and if one should terminate and re-run with new parameters\n",
        "            # The earlier you learn how to read and visualize experiment logs quickly,\n",
        "            # the faster you'll be able to prototype and learn.\n",
        "            #\n",
        "            # Use any debugging information you think you need.\n",
        "            # For instance :\n",
        "\n",
        "            step_counts.append(self.episode_length)\n",
        "            mean_steps = np.mean(step_counts[-100:])\n",
        "            avg_100_steps.append(mean_steps)\n",
        "#             print(\n",
        "#                 \"Training: Episode = %d, Length = %d, Global step = %d, Last-100 mean steps = %d\"\n",
        "#                 % (episode, self.episode_length, total_steps, mean_steps))\n",
        "            # self.summary(value=[tf.Summary.Value(tag=\"episode length\", simple_value=episode_length),])\n",
        "            # self.summary_writer.add_summary(self.summary, episode)\n",
        "\n",
        "            # summary1(value=[\n",
        "            #     tf.Summary.Value(\n",
        "            #         tag=\"episode length\", simple_value=episode_length),\n",
        "            # ])\n",
        "            # summary_writer1.add_summary(summary2, episode)\n",
        "\n",
        "        saver.save(self.session, 'models/my_model')\n",
        "        return avg_100_steps, step_counts\n",
        "\n",
        "    # Simple function to visually 'test' a policy\n",
        "    def playPolicy(self):\n",
        "        # Restore variables from disk.\n",
        "        # saver = tf.train.Saver()\n",
        "        # saver.restore(self.session, \"/models/model.ckpt\")\n",
        "        # print(\"Model restored.\")\n",
        "\n",
        "        done = False\n",
        "        steps = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
        "        while not done and steps < 200:\n",
        "            # self.env.render()\n",
        "            q_vals = self.session.run(self.Q, feed_dict={self.x: [state]})\n",
        "            action = q_vals.argmax()\n",
        "            state, _, done, _ = self.env.step(action)\n",
        "            steps += 1\n",
        "\n",
        "        return steps\n",
        "\n",
        "    # Simple function to visually 'test' a policy\n",
        "    def playPolicyFromSavedModel(self):\n",
        "        # Restore variables from disk.\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(self.session, \"/models/model.ckpt\")\n",
        "        print(\"Model restored.\")\n",
        "        graph = tf.get_default_graph()\n",
        "        self.x = graph.get_tensor_by_name(\"x:0\")\n",
        "        self.Q = graph.get_tensor_by_name(\"Q_values:0\")\n",
        "        # self.Q = tf.add(tf.matmul(h2, W3), b3)\n",
        "        done = False\n",
        "        steps = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
        "        while not done and steps < 200:\n",
        "            # self.env.render()\n",
        "            q_vals = self.session.run(self.Q, feed_dict={self.x: [state]})\n",
        "            action = q_vals.argmax()\n",
        "            state, _, done, _ = self.env.step(action)\n",
        "            steps += 1\n",
        "\n",
        "        return steps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA2X-R5PnbQw",
        "colab_type": "code",
        "outputId": "e1b2a75d-5ee5-4a51-e19e-ebfa6c396a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# Create and initialize the model\n",
        "dqn = DQN('CartPole-v0')\n",
        "# dqn.load_model = True\n",
        "dqn.initialize_network()\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "avg_100_steps, step_counts = dqn.train()\n",
        "print(\"\\nFinished training...\\nCheck out some demonstrations\\n\")\n",
        "results = []\n",
        "for i in range(100):\n",
        "        episode_length = dqn.playPolicy()\n",
        "        print(\"Test steps = \", episode_length)\n",
        "        results.append(episode_length)\n",
        "print(\"Mean steps = \", sum(results) / len(results))\n",
        "\n",
        "print(\"\\nFinished.\")\n",
        "print(\"\\nCiao, and hasta la vista...\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Starting training...\n",
            "\n",
            "INFO:tensorflow:Summary name Episode Length is illegal; using Episode_Length instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/10000 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AxisError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d37a7105899f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mavg_100_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinished training...\\nCheck out some demonstrations\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7f5798e25888>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes_num)\u001b[0m\n\u001b[1;32m    269\u001b[0m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m                     \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m                     \u001b[0mmax_q_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                     \u001b[0;31m# Computing Target Q Values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m     26\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     27\u001b[0m           initial=_NoValue):\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
            "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jre4ZI9IUM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6d2kX-YNLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Average total reward of last 100-episodes\n",
        "x = np.arange(dqn.EPISODES_NUM)\n",
        "plt.clf()\n",
        "plt.plot(x, avg_100_steps)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average Steps for given Episode\")\n",
        "plt.savefig('gdrive/My Drive/AvgNOrelay_rewards993010050064.png', dpi=200)\n",
        "\n",
        "    # Plotting Episode Lengths\n",
        "plt.clf()\n",
        "plt.plot(x, step_counts)\n",
        "plt.title(\"Episode Lengths\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.savefig('gdrive/My Drive/NOrelayEpisode_lengths993010050064.png', dpi=200)\n",
        "\n",
        "    # Plotting Learned agent episodes length for 100 plays\n",
        "x = np.arange(100)\n",
        "plt.clf()\n",
        "plt.plot(x, results)\n",
        "plt.title(\"Learned agent episodes length for 100 plays\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.savefig('gdrive/My Drive/NO relayLearned_Episode_lengths993010050064.png', dpi=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZHljOql_1CD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "6ad27990-0e0f-4806-c202-dd5a85e889bd"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Untitled7.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1STjjZvsb81Xa1CL6zjdNREuVCFYKF0gm\n",
        "\"\"\"\n",
        "\n",
        "import gym\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tqdm\n",
        "\n",
        "# Referred to viswanathgs git 'simple_dqn.py' at https://gist.github.com/viswanathgs/abe4a8732a81c666af8bb99254b8e1da\n",
        "# for defining loss function with target newtwork for DQN \n",
        "\n",
        "class DQN:\n",
        "    np.random.seed(1402)\n",
        "    REPLAY_MEMORY_SIZE = 10000    # number of tuples in experience replay\n",
        "    EPSILON = 1       # epsilon of epsilon-greedy exploation\n",
        "    EPSILON_DECAY = 0.995   # exponential decay multiplier for epsilon\n",
        "    MIN_EPSILON = 0.01  # Final minimum value of epsilon in epsilon-greedy\n",
        "    HIDDEN1_SIZE = 64      # size of hidden layer 1\n",
        "    HIDDEN2_SIZE = 64     # size of hidden layer 2\n",
        "    EPISODES_NUM = 500  # number of episodes to train on. Ideally shouldn't take longer than 2000\n",
        "    MAX_STEPS = 200      # maximum number of steps in an episode\n",
        "    LEARNING_RATE = 0.001     # learning rate and other parameters for SGD/RMSProp/Adam\n",
        "    MOMENTUM = 0.95\n",
        "    MINIBATCH_SIZE = 20    # size of minibatch sampled from the experience replay\n",
        "    DISCOUNT_FACTOR = 0.999     # MDP's gamma\n",
        "    TARGET_UPDATE_FREQ = 200    # number of steps (not episodes) after which to update the target networks\n",
        "    REGULARIZATION_FACTOR = 0.0001\n",
        "    replay_memory = []\n",
        "\n",
        "    # Create and initialize the environment\n",
        "    def __init__(self, env):\n",
        "        self.env = gym.make(env)\n",
        "        assert len(self.env.observation_space.shape) == 1\n",
        "        # In case of cartpole, 4 state features\n",
        "        self.input_size = self.env.observation_space.shape[0]\n",
        "        # In case of cartpole, 2 actions (right/left)\n",
        "        self.output_size = self.env.action_space.n\n",
        "        self.load_model = False\n",
        "        self.session = tf.Session()\n",
        "\n",
        "    # Create the Q-network\n",
        "    def initialize_network(self):\n",
        "\n",
        "        # placeholder for the state-space input to the q-network\n",
        "        tf.set_random_seed(1402)\n",
        "        if 0:\n",
        "            # new_saver = tf.train.import_meta_graph('models/my_model.meta')\n",
        "            # new_saver.restore(self.session,\n",
        "            #                   tf.train.latest_checkpoint('models/./'))\n",
        "            print(\"Model and data loaded \\n\")\n",
        "        else:\n",
        "            self.x = tf.placeholder(\n",
        "                tf.float32, [None, self.input_size], name=\"x\")\n",
        "            self.episode_length = tf.placeholder(\n",
        "                \"float\", name=\"episode_length\")\n",
        "\n",
        "            ############################################################\n",
        "            # Design your q-network here.\n",
        "            #\n",
        "            # Add hidden layers and the output layer. For instance:\n",
        "            #\n",
        "            # with tf.name_scope('output'):\n",
        "            #\tW_n = tf.Variable(\n",
        "            # \t\t\t tf.truncated_normal([self.HIDDEN_n-1_SIZE, self.output_size],\n",
        "            # \t\t\t stddev=0.01), name='W_n')\n",
        "            # \tb_n = tf.Variable(tf.zeros(self.output_size), name='b_n')\n",
        "            # \tself.Q = tf.matmul(h_n-1, W_n) + b_n\n",
        "            #\n",
        "            #############################################################\n",
        "\n",
        "            # Your code here\n",
        "            with tf.name_scope('hidden1'):\n",
        "                W1 = tf.Variable(tf.random_uniform(\n",
        "                    [self.input_size, self.HIDDEN1_SIZE], -1.0, 1.0), name='W1')\n",
        "                b1 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE], -1.0, 1.0), name='b1')\n",
        "                h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n",
        "\n",
        "            with tf.name_scope('hidden2'):\n",
        "                W2 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE, self.HIDDEN2_SIZE], -1.0, 1.0), name='W2')\n",
        "                b2 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN1_SIZE], -1.0, 1.0), name='b2')\n",
        "                h2 = tf.nn.relu(tf.matmul(h1, W2) + b2)\n",
        "\n",
        "            with tf.name_scope('output'):\n",
        "                W3 = tf.Variable(tf.random_uniform(\n",
        "                    [self.HIDDEN2_SIZE, self.output_size], -1.0, 1.0), name='W4')\n",
        "                b3 = tf.Variable(tf.random_uniform(\n",
        "                    [self.output_size], -1.0, 1.0), name='b4')\n",
        "                # , name=\"Q_values\")\n",
        "                self.Q = tf.squeeze(tf.matmul(h2, W3) + b3)\n",
        "\n",
        "            self.weights = [W1, b1, W2, b2,  W3, b3]\n",
        "\n",
        "            ############################################################\n",
        "            # Next, compute the loss.\n",
        "            #\n",
        "            # First, compute the q-values. Note that you need to calculate these\n",
        "            # for the actions in the (s,a,s',r) tuples from the experience replay's minibatch\n",
        "            #\n",
        "            # Next, compute the l2 loss between these estimated q-values and\n",
        "            # the target (which is computed using the frozen target network)\n",
        "            #\n",
        "            ############################################################\n",
        "\n",
        "            # Your code here\n",
        "            self.targetQ = tf.placeholder(tf.float32, [None])\n",
        "            self.targetActionMask = tf.placeholder(\n",
        "                tf.float32, [None, self.output_size])\n",
        "            q_values = tf.reduce_sum(tf.multiply(\n",
        "                self.Q, self.targetActionMask), reduction_indices=[1])\n",
        "            self.loss = tf.reduce_mean(\n",
        "                tf.square(tf.subtract(q_values, self.targetQ)))\n",
        "\n",
        "            # # Regularization\n",
        "            for w in [W1, W2]:\n",
        "                self.loss += self.REGULARIZATION_FACTOR * \\\n",
        "                    tf.reduce_sum(tf.square(w))\n",
        "\n",
        "                ############################################################\n",
        "                # Finally, choose a gradient descent algorithm : SGD/RMSProp/Adam.\n",
        "                #\n",
        "                # For instance:\n",
        "                # optimizer = tf.train.GradientDescentOptimizer(self.LEARNING_RATE)\n",
        "                # global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                # self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                optimizer = tf.train.AdamOptimizer(learning_rate=self.LEARNING_RATE)\n",
        "                global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "                self.train_op = optimizer.minimize(self.loss, global_step=global_step)\n",
        "\n",
        "        ############################################################\n",
        "\n",
        "    def train(self, episodes_num=EPISODES_NUM):\n",
        "\n",
        "        # Initialize the TF session\n",
        "        saver = tf.train.Saver()\n",
        "        self.episode_length = 0\n",
        "\n",
        "        # Initialize summary for TensorBoard\n",
        "        tf.summary.scalar('loss',self.loss)\n",
        "        tf.summary.scalar('Episode Length', self.episode_length)\n",
        "        self.summary = tf.summary.merge_all()\n",
        "        self.summary_writer = tf.summary.FileWriter(self.LOG_DIR, self.session.graph)\n",
        "\n",
        "\n",
        "        # summary_writer1 = tf.summary.FileWriter(self.LOG_DIR2)\n",
        "        # summary1 = tf.Summary()\n",
        "\n",
        "        # Alternatively, you could use animated real-time plots from matplotlib\n",
        "        # (https://stackoverflow.com/a/24228275/3284912)\n",
        "\n",
        "        self.session.run(tf.global_variables_initializer())\n",
        "        # saver = tf.train.Saver()\n",
        "        saver.save(self.session, 'models/my_model')\n",
        "\n",
        "        ############################################################\n",
        "        # Initialize other variables (like the replay memory)\n",
        "        ############################################################\n",
        "\n",
        "        # Your code here\n",
        "        total_steps = 0\n",
        "        step_counts = []\n",
        "        avg_100_steps = []\n",
        "\n",
        "        # Copying weight to target network\n",
        "        target_weights = self.session.run(self.weights)\n",
        "\n",
        "        ############################################################\n",
        "        # Main training loop\n",
        "        #\n",
        "        # In each episode,\n",
        "        #\tpick the action for the given state,\n",
        "        #\tperform a 'step' in the environment to get the reward and next state,\n",
        "        #\tupdate the replay buffer,\n",
        "        #\tsample a random minibatch from the replay buffer,\n",
        "        # \tperform Q-learning,\n",
        "        #\tupdate the target network, if required.\n",
        "        #\n",
        "        #\n",
        "        #\n",
        "        # You'll need to write code in various places in the following skeleton\n",
        "        #\n",
        "        ############################################################\n",
        "\n",
        "        for episode in tqdm.tqdm(range(episodes_num)):\n",
        "\n",
        "            state = self.env.reset()\n",
        "\n",
        "            ############################################################\n",
        "            # Episode-specific initializations go here.\n",
        "            ############################################################\n",
        "            #\n",
        "            # Your code here\n",
        "            #\n",
        "            self.episode_length = 0\n",
        "            # done = False\n",
        "\n",
        "            ############################################################\n",
        "            # (done == False) and (episode_length < self.MAX_STEPS):\n",
        "            while True:\n",
        "                # print(done)\n",
        "                # for step in range(self.MAX_STEPS):\n",
        "                ############################################################\n",
        "                # Pick the next action using epsilon greedy and and execute it\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                action = None\n",
        "                if self.EPSILON > random.random():\n",
        "                    action = self.env.action_space.sample()\n",
        "                else:\n",
        "                    q_values = self.session.run(\n",
        "                        self.Q, feed_dict={self.x: [state]})\n",
        "                    action = q_values.argmax()\n",
        "\n",
        "                # Decaying epsilon\n",
        "                self.EPSILON *= self.EPSILON_DECAY\n",
        "                if self.EPSILON < self.MIN_EPSILON:\n",
        "                    self.EPSILON = self.MIN_EPSILON\n",
        "\n",
        "\n",
        "                ############################################################\n",
        "                # Step in the environment. Something like:\n",
        "                # next_state, reward, done, _ = self.env.step(action)\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                ############################################################\n",
        "                # Update the (limited) replay buffer.\n",
        "                #\n",
        "                # Note : when the replay buffer is full, you'll need to\n",
        "                # remove an entry to accommodate a new one.\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                if done:\n",
        "                    reward = -100\n",
        "\n",
        "                self.replay_memory.append(\n",
        "                    (state, action, reward, next_state, done))\n",
        "\n",
        "                # removing an entry if replay buffer is full\n",
        "                if len(self.replay_memory) > self.REPLAY_MEMORY_SIZE:\n",
        "                    self.replay_memory.pop(0)\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                ############################################################\n",
        "                # Sample a random minibatch and perform Q-learning (fetch max Q at s')\n",
        "                #\n",
        "                # Remember, the target (r + gamma * max Q) is computed\n",
        "                # with the help of the target network.\n",
        "                # Compute this target and pass it to the network for computing\n",
        "                # and minimizing the loss with the current estimates\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                if len(self.replay_memory) >= self.MINIBATCH_SIZE:\n",
        "                    minibatch = random.sample(\n",
        "                        self.replay_memory, self.MINIBATCH_SIZE)\n",
        "                    next_states = [s[3] for s in minibatch]\n",
        "\n",
        "                    feed_dict = {self.x: next_states}\n",
        "                    feed_dict.update(zip(self.weights, target_weights))\n",
        "                    q_values = self.session.run(self.Q, feed_dict=feed_dict)\n",
        "                    max_q_values = q_values.max(axis=1)\n",
        "\n",
        "                    # Computing Target Q Values\n",
        "                    target_q = np.zeros(self.MINIBATCH_SIZE)\n",
        "                    target_action_mask = np.zeros(\n",
        "                        (self.MINIBATCH_SIZE, self.output_size), dtype=int)\n",
        "                    for i in range(self.MINIBATCH_SIZE):\n",
        "                        st, action, reward, n_st, completed = minibatch[i]\n",
        "                        target_q[i] = reward\n",
        "                        if not completed:\n",
        "                            target_q[i] += self.DISCOUNT_FACTOR*max_q_values[i]\n",
        "                        target_action_mask[i][action] = 1\n",
        "\n",
        "                    # Performing gradient descent\n",
        "                    states = [s[0] for s in minibatch]\n",
        "                    feed_dict = {\n",
        "                        self.x: states,\n",
        "                        self.targetQ: target_q,\n",
        "                        self.targetActionMask: target_action_mask,\n",
        "                    }\n",
        "                    _, summary2 = self.session.run(\n",
        "                        [self.train_op, self.summary], feed_dict=feed_dict)\n",
        "                    \n",
        "                    ############################################################\n",
        "                    # Update target weights.\n",
        "                    #\n",
        "                    # Something along the lines of:\n",
        "                    # if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
        "                    # \ttarget_weights = self.session.run(self.weights)\n",
        "                    ############################################################\n",
        "\n",
        "                    # Your code here\n",
        "                    if total_steps % self.TARGET_UPDATE_FREQ == 0:\n",
        "                        target_weights = self.session.run(self.weights)\n",
        "                        #print(\"Targets updated \\n\")\n",
        "\n",
        "                    if total_steps % 100 == 0:\n",
        "                        self.summary_writer.add_summary(summary2, episode)\n",
        "\n",
        "\n",
        "                        # Save the variables to disk.\n",
        "                        # save_path = saver.save(self.session, \"/models/model.ckpt\")\n",
        "                        # print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "\n",
        "                ############################################################\n",
        "                # Break out of the loop if the episode ends\n",
        "                #\n",
        "                # Something like:\n",
        "                # if done or (episode_length == self.MAX_STEPS):\n",
        "                # \tbreak\n",
        "                #\n",
        "                ############################################################\n",
        "\n",
        "                # Your code here\n",
        "                total_steps += 1\n",
        "                self.episode_length += 1\n",
        "\n",
        "                if done or (self.episode_length == self.MAX_STEPS):\n",
        "                    break\n",
        "\n",
        "            ############################################################\n",
        "            # Logging.\n",
        "            #\n",
        "            # Very important. This is what gives an idea of how good the current\n",
        "            # experiment is, and if one should terminate and re-run with new parameters\n",
        "            # The earlier you learn how to read and visualize experiment logs quickly,\n",
        "            # the faster you'll be able to prototype and learn.\n",
        "            #\n",
        "            # Use any debugging information you think you need.\n",
        "            # For instance :\n",
        "\n",
        "            step_counts.append(self.episode_length)\n",
        "            mean_steps = np.mean(step_counts[-100:])\n",
        "            avg_100_steps.append(mean_steps)\n",
        "#             print(\n",
        "#                 \"Training: Episode = %d, Length = %d, Global step = %d, Last-100 mean steps = %d\"\n",
        "#                 % (episode, self.episode_length, total_steps, mean_steps))\n",
        "            # self.summary(value=[tf.Summary.Value(tag=\"episode length\", simple_value=episode_length),])\n",
        "            # self.summary_writer.add_summary(self.summary, episode)\n",
        "\n",
        "            # summary1(value=[\n",
        "            #     tf.Summary.Value(\n",
        "            #         tag=\"episode length\", simple_value=episode_length),\n",
        "            # ])\n",
        "            # summary_writer1.add_summary(summary2, episode)\n",
        "\n",
        "        saver.save(self.session, 'models/my_model')\n",
        "        return avg_100_steps, step_counts\n",
        "\n",
        "    # Simple function to visually 'test' a policy\n",
        "    def playPolicy(self):\n",
        "        # Restore variables from disk.\n",
        "        # saver = tf.train.Saver()\n",
        "        # saver.restore(self.session, \"/models/model.ckpt\")\n",
        "        # print(\"Model restored.\")\n",
        "\n",
        "        done = False\n",
        "        steps = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
        "        while not done and steps < 200:\n",
        "            # self.env.render()\n",
        "            q_vals = self.session.run(self.Q, feed_dict={self.x: [state]})\n",
        "            action = q_vals.argmax()\n",
        "            state, _, done, _ = self.env.step(action)\n",
        "            steps += 1\n",
        "\n",
        "        return steps\n",
        "\n",
        "    # Simple function to visually 'test' a policy\n",
        "    def playPolicyFromSavedModel(self):\n",
        "        # Restore variables from disk.\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(self.session, \"/models/model.ckpt\")\n",
        "        print(\"Model restored.\")\n",
        "        graph = tf.get_default_graph()\n",
        "        self.x = graph.get_tensor_by_name(\"x:0\")\n",
        "        self.Q = graph.get_tensor_by_name(\"Q_values:0\")\n",
        "        # self.Q = tf.add(tf.matmul(h2, W3), b3)\n",
        "        done = False\n",
        "        steps = 0\n",
        "        state = self.env.reset()\n",
        "\n",
        "        # we assume the CartPole task to be solved if the pole remains upright for 200 steps\n",
        "        while not done and steps < 200:\n",
        "            # self.env.render()\n",
        "            q_vals = self.session.run(self.Q, feed_dict={self.x: [state]})\n",
        "            action = q_vals.argmax()\n",
        "            state, _, done, _ = self.env.step(action)\n",
        "            steps += 1\n",
        "\n",
        "        return steps\n",
        "\n",
        "# Create and initialize the model\n",
        "dqn = DQN('CartPole-v0')\n",
        "# dqn.load_model = True\n",
        "dqn.initialize_network()\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "avg_100_steps, step_counts = dqn.train()\n",
        "print(\"\\nFinished training...\\nCheck out some demonstrations\\n\")\n",
        "results = []\n",
        "for i in range(100):\n",
        "        episode_length = dqn.playPolicy()\n",
        "        print(\"Test steps = \", episode_length)\n",
        "        results.append(episode_length)\n",
        "print(\"Mean steps = \", sum(results) / len(results))\n",
        "\n",
        "print(\"\\nFinished.\")\n",
        "print(\"\\nCiao, and hasta la vista...\\n\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Plotting Average total reward of last 100-episodes\n",
        "x = np.arange(dqn.EPISODES_NUM)\n",
        "plt.clf()\n",
        "plt.plot(x, avg_100_steps)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Average Steps for given Episode\")\n",
        "plt.savefig('gdrive/My Drive/Avg_rewards993010050064.png', dpi=200)\n",
        "\n",
        "    # Plotting Episode Lengths\n",
        "plt.clf()\n",
        "plt.plot(x, step_counts)\n",
        "plt.title(\"Episode Lengths\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.savefig('gdrive/My Drive/Episode_lengths993010050064.png', dpi=200)\n",
        "\n",
        "    # Plotting Learned agent episodes length for 100 plays\n",
        "x = np.arange(100)\n",
        "plt.clf()\n",
        "plt.plot(x, results)\n",
        "plt.title(\"Learned agent episodes length for 100 plays\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Episode Length\")\n",
        "plt.savefig('gdrive/My Drive/Learned_Episode_lengths993010050064.png', dpi=200)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "\n",
            "INFO:tensorflow:Summary name Episode Length is illegal; using Episode_Length instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2414bb736447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m \u001b[0mavg_100_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinished training...\\nCheck out some demonstrations\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2414bb736447>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes_num)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode Length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'LOG_DIR'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv6MYVSwVEej",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "81abd92e-18be-4a41-c57e-14d063964a7d"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "\n",
            "Starting training...\n",
            "\n",
            "INFO:tensorflow:Summary name Episode Length is illegal; using Episode_Length instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 15%|        | 76/500 [00:07<02:05,  3.37it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-21b26a4c035b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting training...\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m \u001b[0mavg_100_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFinished training...\\nCheck out some demonstrations\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-21b26a4c035b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes_num)\u001b[0m\n\u001b[1;32m    304\u001b[0m                     }\n\u001b[1;32m    305\u001b[0m                     _, summary2 = self.session.run(\n\u001b[0;32m--> 306\u001b[0;31m                         [self.train_op, self.summary], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0;31m############################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgKF2mYWuyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}